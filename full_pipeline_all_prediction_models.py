# -*- coding: utf-8 -*-
"""ML Final Exam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k2KnW8_feHup1G7IB-OdAgv8ckzdQRkS
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pickle

from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import PolynomialFeatures
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import FunctionTransformer


from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import VotingRegressor, StackingRegressor
from xgboost import XGBRegressor
from sklearn.ensemble import AdaBoostRegressor

from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

import time
import warnings
warnings.filterwarnings("ignore")

"""# **Data Loading**"""

df = pd. read_csv("/content/Medical Insurance Cost.csv")

df.head()

!pip install ydata-profiling

from ydata_profiling import ProfileReport

profile = ProfileReport( df , title="Medical Insurance Cost.csv", explorative = True  )

profile.to_file("ydata.html")

"""# **Data Preprocessing**"""

print('Shape: ', df.shape)
print('\nDuplicate: ',df.duplicated().sum())

df = df.drop_duplicates()

print('Shape: ', df.shape)
print('\nColumns Name: ', df.columns)
print('\nTarget Variable: charges')

display(df.info())
display(df.describe())
display(df.isnull().sum())
display(df.nunique())

target_col = 'charges'

numerical_cols = ['age', 'bmi', 'children']

categorical_cols = ['sex', 'smoker', 'region']

corr_matrix = df[numerical_cols + [target_col]].corr()

plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm",fmt=".2f", linewidths=0.5)
plt.title("Correlation Heatmap for Neumeric Features")
plt.tight_layout()
plt.show()

print(corr_matrix[target_col].sort_values(ascending=False))

df[numerical_cols].hist(bins=20, figsize=(10, 6))
plt.tight_layout()
plt.show()

for c in categorical_cols:
    plt.figure(figsize=(6, 4))
    sns.countplot(data=df, x=c)
    plt.title(f"Countplot of {c}")
    plt.show()

# Boxplot
plt.figure(figsize=(8, 6))
sns.boxplot(data=df[numerical_cols])
plt.title("Numeric features before scaling")
plt.tight_layout()
plt.show()

# Outliers in BMI

Q1 = df['bmi'].quantile(0.25)
Q3 = df['bmi'].quantile(0.75)

IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

print("Lower bound", lower_bound)
print("Upper bound", upper_bound)

outliers = ((df['bmi'] < lower_bound) | (df['bmi'] > upper_bound))
outliers_df = df[outliers]

print(f"\nNumber of outliers detected: {len(outliers_df)}")
display(outliers_df)

# Log-transform
df['bmi_log'] = np.log(df['bmi'] + 1)
df.head()

# Feature Engineering:

#BMI per Age
df['bmi_per_age'] = df['bmi'] / df['age']


# Binning Age
df['age_group'] = pd.cut(
    df['age'],
    bins = [18, 25, 45, 65],
    labels = ['Young', 'Adult', 'Senior'], right=False
)

df.head()

"""# **Pipeline Creation**"""

# Target and features
X = df.drop('charges', axis= 1)
y = df['charges']

# Column split
numerical_cols = ['age', 'children', 'bmi_per_age', 'bmi_log']
categorical_cols_for_label_encoding = ['sex', 'smoker']
categorical_cols_for_one_hot_encoding = ['region', 'age_group']

#Preprocessing
num_transformer = Pipeline (
    steps = [
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
] )

cat_label_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('label_encoder', FunctionTransformer(lambda X: np.array([LabelEncoder().fit_transform(col) for col in X.T]).T))
])

cat_one_hot_transformer = Pipeline( steps = [
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('one_hot_encoder', OneHotEncoder(handle_unknown='ignore'))
] )

preprocessor = ColumnTransformer(
    transformers= [
        ('num',num_transformer,numerical_cols),
        ('cat_label',cat_label_transformer,categorical_cols_for_label_encoding),
        ('cat_one_hot',cat_one_hot_transformer,categorical_cols_for_one_hot_encoding)
] )

"""# **Model Selections**
As this is a supervised regression problem, I choose some regression models.
"""

# Models

mlr = LinearRegression()
plr = Pipeline([
    ('poly', PolynomialFeatures(degree=3)),
    ('linear', LinearRegression())
])
svr = SVR(kernel='rbf', C= 1000000, epsilon= 0.1, gamma= 'scale')
knn = KNeighborsRegressor(n_neighbors= 10, metric= 'minkowski', p= 1, weights= 'uniform')
dt = DecisionTreeRegressor(max_depth= 7, random_state=42)
rf = RandomForestRegressor(n_estimators= 250, max_depth= 7, random_state=42)
ab = AdaBoostRegressor(estimator= DecisionTreeRegressor(max_depth= 7, random_state=42), n_estimators= 250, learning_rate= 0.1, random_state=42)
gb = GradientBoostingRegressor(n_estimators=250, max_depth=3, learning_rate= 0.1,  random_state=42)
xgb = XGBRegressor(n_estimators=250, max_depth=3, learning_rate=0.1, subsample=0.8, colsample_bytree=0.8, objective="reg:squarederror", eval_metric="rmse", random_state=42)

# Voting Regressor
voting_reg = VotingRegressor(estimators=[('rf', rf), ('ab', ab), ('gb', gb), ('xgb', xgb)])

# Stacking Regressor
stacking_reg = StackingRegressor(estimators=[('rf', rf), ('ab', ab),  ('gb', gb), ('xgb', xgb)], final_estimator=Ridge())

# Dictionary of all models
model_to_train = {
    'Linear Regression': mlr,
    'Polynomial Regression': plr,
    'SVR': svr,
    'KNN': knn,
    'Decision Tree': dt,
    'Random Forest': rf,
    'AdaBoost': ab,
    'Gradient Boosting': gb,
    'XGBoost': xgb,
    'Voting Ensemble': voting_reg,
    'Stacking Ensemble': stacking_reg
}

# Train-test split
X_train,X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2 , random_state=42)

"""# **Model Training**"""

# Training & Evaluation

result = []

for name, model in model_to_train.items():
    start_time = time.time()

    pipe = Pipeline(
        [
            ('preprocessor', preprocessor),
            ('model', model)
        ]
    )

    # Training
    pipe.fit(X_train, y_train)

    # Measure elapsed time
    elapsed_time = time.time() - start_time

    # Predict
    y_train_pred = pipe.predict(X_train)
    y_pred = pipe.predict(X_test)

    # Train Evaluation
    r2_train = r2_score(y_train, y_train_pred)

    # Test Evaluation
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)

    result.append({
        "Model": name,
        "R2 Score Train": r2_train,
        "R2 Score": r2,
        "R2 Difference": r2_train - r2,
        "RMSE": rmse,
        "MAE": mae,
        "Time to Train (seconds)": elapsed_time
    })

results_df = pd.DataFrame(result).sort_values("R2 Score", ascending=False)
results_df

"""Now, I choose XGBoost Regressor as Primary model for further analysis by CV, Hyperparameter tuning, beacuse this provide the most balance performance between training and tesing score and requires less computational time than other, especially Random Forest and Gradient Boosting.

# **Cross Validation**
"""

xgb_pipeline = Pipeline(
    [
        ('preprocessor', preprocessor),
        ('model', XGBRegressor(
            n_estimators=250,
            max_depth=3,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            objective="reg:squarederror",
            eval_metric="rmse",
            random_state=42
        ))
    ]
)


xgb_cv_scores = cross_val_score(xgb_pipeline, X_train, y_train, cv=5, scoring='neg_mean_squared_error')
xgb_cv_rmse = np.sqrt(-xgb_cv_scores)

print("XGBoost CV RMSE: ", xgb_cv_rmse)
print("Mean CV RMSE (XGBoost): ", xgb_cv_rmse.mean())
print("Standard Deviation CV RMSE (XGBoost): ", xgb_cv_rmse.std())

"""# **Hyperparameter Tuning**"""

# RandomizedSearchCV

xgb_pipeline = Pipeline(
    [
        ('preprocessor', preprocessor),
        ('model', XGBRegressor(random_state=42))
    ]
)

param_dist = {
    'model__n_estimators': [100, 200, 250, 300, 350, 400],
    'model__max_depth': [3, 5, 7, 10, 15],
    'model__learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],
    'model__subsample': [0.7, 0.8, 0.9, 1.0],
    'model__colsample_bytree': [0.7, 0.8, 0.9, 1.0]
}

random_search_xgb = RandomizedSearchCV(
    estimator=xgb_pipeline,
    param_distributions=param_dist,
    n_iter=50,
    cv=5,
    scoring='neg_root_mean_squared_error',
    n_jobs=-1,
    verbose=1,
    random_state=42
)


random_search_xgb.fit(X_train, y_train)

print("Best Parameters: ", random_search_xgb.best_params_)
print("Best Score: ", -random_search_xgb.best_score_)


# Best model from RandomizedSearchCV
best_xgb_model = random_search_xgb.best_estimator_
y_pred = best_xgb_model.predict(X_test)

# Evaluation
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)

print("\nRandomizedSearchCV Best Scores: ")
print("Best R2 Score: ", r2)
print("Best RMSE: ", rmse)
print("Best MAE: ", mae)

#Grid Search CV

xgb_pipeline = Pipeline(
    [
        ('preprocessor', preprocessor),
        ('model', XGBRegressor(random_state=42))
    ]
)


param_grid = {
    'model__n_estimators': [100, 200, 250, 300],
    'model__max_depth': [3, 5, 7, 10],
    'model__learning_rate': [0.01, 0.05, 0.1, 0.3],
    'model__subsample': [0.7, 0.8, 0.9],
    'model__colsample_bytree': [0.7, 0.8, 0.9]
}


grid_search_xgb = GridSearchCV(
    estimator=xgb_pipeline,
    param_grid=param_grid,
    cv=5,
    scoring='neg_root_mean_squared_error',
    n_jobs=-1,
    verbose=1
)


grid_search_xgb.fit(X_train, y_train)

print("Best Parameters: ", grid_search_xgb.best_params_)
print("Best Score: ", -grid_search_xgb.best_score_)

# Best model from GridSearchCV
best_xgb_model = grid_search_xgb.best_estimator_
y_pred = best_xgb_model.predict(X_test)

# Evaluation
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)

print("\nGridSearchCV Best Scores: ")
print("Best R2 Score: ", r2)
print("Best RMSE: ", rmse)
print("Best MAE: ", mae)

"""# **Best Model Selection with Evaluation**

Best model XGBBoost Regressor's
Best Parameters:  {'model__colsample_bytree': 0.8, 'model__learning_rate': 0.05, 'model__max_depth': 3, 'model__n_estimators': 100, 'model__subsample': 0.8}
"""

# Best Model

best_model_xgb = XGBRegressor(
    n_estimators=100,
    max_depth=3,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    objective="reg:squarederror",
    eval_metric="rmse",
    random_state=42,
    n_jobs=-1
)

best_model_xgb_pipeline = Pipeline(
    [
        ('preprocessor', preprocessor),
        ('model', best_model_xgb)
    ])

best_model_xgb_pipeline.fit(X_train, y_train)

y_pred = best_model_xgb_pipeline.predict(X_test)
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)

print("Best Model Scores: ")
print("R2 Score: ", r2)
print("RMSE: ", rmse)
print("MAE: ", mae)

# Save Best Model
!pip install cloudpickle
import cloudpickle

with open('best_model_xgb.pkl', 'wb') as f:
    cloudpickle.dump(best_model_xgb_pipeline, f)

print("✅ Best model saved as best_model_xgb.pkl")

# Actual vs Predicted plot
plt.figure(figsize=(6, 4))
sns.scatterplot(x=y_test, y=y_pred, alpha=0.6, color='teal')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')
plt.xlabel("Actual Insurance Charges")
plt.ylabel("Predicted Insurance Charges")
plt.title("Actual vs Predicted Insurance Charges")
plt.grid(True)
plt.show()

"""# **Preapare the Best Model for Web Interface**"""

import pandas as pd
import numpy as np
import cloudpickle
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import FunctionTransformer
from sklearn.pipeline import Pipeline
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import matplotlib.pyplot as plt
import seaborn as sns

# =====================
# Load dataset
# =====================
df = pd.read_csv("/content/Medical Insurance Cost.csv")
df.head()

# Drop duplicates
print('Duplicate: ',df.duplicated().sum())
df = df.drop_duplicates()

# Display Info
print(df.info())
print(df.describe())
print(df.isnull().sum())
print(df.nunique())

# =====================
# Outlier
# =====================

# Outliers in BMI
Q1 = df['bmi'].quantile(0.25)
Q3 = df['bmi'].quantile(0.75)

IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

print("Lower bound", lower_bound)
print("Upper bound", upper_bound)

outliers = ((df['bmi'] < lower_bound) | (df['bmi'] > upper_bound))
outliers_df = df[outliers]

print(f"\nNumber of outliers detected: {len(outliers_df)}")
print(outliers_df)

# =====================
# Feature Engineering
# =====================
# Log-transform bmi
df['bmi_log'] = np.log(df['bmi'] + 1)

# BMI per Age
df['bmi_per_age'] = df['bmi'] / df['age']

# Binning Age
df['age_group'] = pd.cut(
    df['age'],
    bins=[18, 25, 45, 65],
    labels=['Young', 'Adult', 'Senior'], right=False
)

# Target and features
X = df.drop('charges', axis=1)
y = df['charges']

# =====================
# Column split
# =====================
numerical_cols = ['age', 'children', 'bmi_per_age', 'bmi_log']
categorical_cols_for_label_encoding = ['sex', 'smoker']
categorical_cols_for_one_hot_encoding = ['region', 'age_group']

# =====================
# Preprocessing
# =====================
# Numerical features: impute missing values and scale
num_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# Categorical features for label encoding
cat_label_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('label_encoder', FunctionTransformer(lambda X: np.array([LabelEncoder().fit_transform(col) for col in X.T]).T))
])

# Categorical features for one-hot encoding
cat_one_hot_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('one_hot_encoder', OneHotEncoder(handle_unknown='ignore'))
])

# Combine all transformers into a preprocessor
preprocessor = ColumnTransformer(transformers=[
    ('num', num_transformer, numerical_cols),
    ('cat_label', cat_label_transformer, categorical_cols_for_label_encoding),
    ('cat_one_hot', cat_one_hot_transformer, categorical_cols_for_one_hot_encoding)
])

# =====================
# Best Model: XGBoost
# =====================
best_model_xgb = XGBRegressor(
    n_estimators=100,
    max_depth=3,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    objective="reg:squarederror",
    eval_metric="rmse",
    random_state=42,
    n_jobs=-1
)

# Create full pipeline with preprocessing and model
best_model_xgb_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', best_model_xgb)
])

# =====================
# Train-test split
# =====================
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# =====================
# Train the model
# =====================
best_model_xgb_pipeline.fit(X_train, y_train)

# =====================
# Evaluation
# =====================
y_pred = best_model_xgb_pipeline.predict(X_test)
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)

print("Best Model Scores: ")
print("R2 Score: ", r2)
print("RMSE: ", rmse)
print("MAE: ", mae)

# =====================
# Save Best Model
# =====================
with open('best_model_xgb.pkl', 'wb') as f:
    cloudpickle.dump(best_model_xgb_pipeline, f)

print("✅ Best model saved as best_model_xgb.pkl")

# =====================
# Plot Actual vs Predicted
# =====================
plt.figure(figsize=(6, 4))
sns.scatterplot(x=y_test, y=y_pred, alpha=0.6, color='teal')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')
plt.xlabel("Actual Insurance Charges")
plt.ylabel("Predicted Insurance Charges")
plt.title("Actual vs Predicted Insurance Charges")
plt.grid(True)
plt.show()